{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Shot Learning with Siamese Networks\n",
    "\n",
    "This is the jupyter notebook that accompanies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "All the imports are defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Set of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(img,text,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Class\n",
    "A simple class to manage configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    training_dir = \"/usr/prakt/w343/Downloads/Photos/training\"\n",
    "    testing_dir = \"/usr/prakt/w343/Downloads/Photos/testing\"\n",
    "    train_batch_size = 16\n",
    "    train_number_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "This dataset generates a pair of images. 0 for geniune pair and 1 for imposter pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training DataSet Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        #img0 = img0.convert(\"L\")\n",
    "        #img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing DataSet Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagSiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = self.imageFolderDataset.imgs\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32)), int(img0_tuple[1]), int(img1_tuple[1]) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Image Folder Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_dataset = dset.ImageFolder(root=Config.training_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/prakt/w343/.venv/lib/python3.5/site-packages/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,\n",
    "                                        transform=transforms.Compose([transforms.Scale((100,100)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising some of the data\n",
    "The top row and the bottom row of any column is one pair. The 0s and 1s correspond to the column of the image.\n",
    "0 indiciates dissimilar, and 1 indicates similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vis_dataloader = DataLoader(siamese_dataset,\n",
    "#                        shuffle=True,\n",
    "#                        num_workers=8,\n",
    "#                        batch_size=2)\n",
    "#dataiter = iter(vis_dataloader)\n",
    "\n",
    "\n",
    "#example_batch = next(dataiter)\n",
    "#concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "#imshow(torchvision.utils.make_grid(concatenated))\n",
    "#print(example_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Net Definition\n",
    "We will use a standard convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(3, 4, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout2d(p=.2),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(4, 8, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout2d(p=.2),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(8, 8, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout2d(p=.2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(8*100*100, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 20))\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=8,\n",
    "                        batch_size=Config.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = SiameseNetwork().cuda()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.0005 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      " Current loss 0.6629258394241333\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 2.7577545642852783\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 2.87920880317688\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.7203068733215332\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.8815017938613892\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.4311052560806274\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.3416295051574707\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 0.9184504151344299\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.6140267848968506\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.0015614032745361\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.191805124282837\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 1.1020044088363647\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.1159759759902954\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 0.9093016386032104\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 0.8919526934623718\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.2477363348007202\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.8789801597595215\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 0.7470971345901489\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.0013518333435059\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.8753306865692139\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.1636923551559448\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 0.9122423529624939\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 0.6737262606620789\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.1608561277389526\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.4199029207229614\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 0.7036925554275513\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 0.9196093678474426\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.0391340255737305\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 0.5644692778587341\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.1328160762786865\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.6144691705703735\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 0.7728329300880432\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 0.8069791793823242\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 0.922188401222229\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 0.7192241549491882\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.4465502500534058\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 0.9946815967559814\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 0.8411498069763184\n",
      "\n",
      "Epoch number 12\n",
      " Current loss 0.8146275877952576\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.649673581123352\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.012487769126892\n",
      "\n",
      "Epoch number 13\n",
      " Current loss 1.8964641094207764\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 0.8367050290107727\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.07987380027771\n",
      "\n",
      "Epoch number 14\n",
      " Current loss 1.1277353763580322\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.1370748281478882\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 1.474740982055664\n",
      "\n",
      "Epoch number 15\n",
      " Current loss 0.924920380115509\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.058738350868225\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.2437443733215332\n",
      "\n",
      "Epoch number 16\n",
      " Current loss 1.6761221885681152\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.1309502124786377\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 0.6420402526855469\n",
      "\n",
      "Epoch number 17\n",
      " Current loss 1.1946169137954712\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 0.9767439961433411\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.396754503250122\n",
      "\n",
      "Epoch number 18\n",
      " Current loss 1.192799687385559\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.2636117935180664\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 1.4962413311004639\n",
      "\n",
      "Epoch number 19\n",
      " Current loss 0.56235671043396\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 0.9912236332893372\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.521070957183838\n",
      "\n",
      "Epoch number 20\n",
      " Current loss 1.1078381538391113\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.1482977867126465\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 0.8537784814834595\n",
      "\n",
      "Epoch number 21\n",
      " Current loss 1.1398663520812988\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.1066588163375854\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.0323659181594849\n",
      "\n",
      "Epoch number 22\n",
      " Current loss 1.2794004678726196\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.0821914672851562\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 1.1624994277954102\n",
      "\n",
      "Epoch number 23\n",
      " Current loss 0.6459648013114929\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 0.8464881777763367\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 0.9360626339912415\n",
      "\n",
      "Epoch number 24\n",
      " Current loss 1.2036899328231812\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 1.0351195335388184\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 0.8255449533462524\n",
      "\n",
      "Epoch number 25\n",
      " Current loss 0.7906743288040161\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.5710813999176025\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.3599841594696045\n",
      "\n",
      "Epoch number 26\n",
      " Current loss 1.1593724489212036\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.2971800565719604\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 1.4355597496032715\n",
      "\n",
      "Epoch number 27\n",
      " Current loss 0.8595755100250244\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.1695528030395508\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 2.712993621826172\n",
      "\n",
      "Epoch number 28\n",
      " Current loss 1.0062308311462402\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 0.900498628616333\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 1.4436140060424805\n",
      "\n",
      "Epoch number 29\n",
      " Current loss 0.71210777759552\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.3164585828781128\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 0.7017861008644104\n",
      "\n",
      "Epoch number 30\n",
      " Current loss 1.0682908296585083\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 0.7336231470108032\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 0.9725947380065918\n",
      "\n",
      "Epoch number 31\n",
      " Current loss 1.1415859460830688\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 0.948309063911438\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 1.0755716562271118\n",
      "\n",
      "Epoch number 32\n",
      " Current loss 1.0208324193954468\n",
      "\n",
      "Epoch number 33\n",
      " Current loss 1.1951699256896973\n",
      "\n",
      "Epoch number 33\n",
      " Current loss 1.5236104726791382\n",
      "\n",
      "Epoch number 33\n",
      " Current loss 1.0754374265670776\n",
      "\n",
      "Epoch number 34\n",
      " Current loss 1.148133635520935\n",
      "\n",
      "Epoch number 34\n",
      " Current loss 0.9151586294174194\n",
      "\n",
      "Epoch number 34\n",
      " Current loss 0.9932424426078796\n",
      "\n",
      "Epoch number 35\n",
      " Current loss 1.376793622970581\n",
      "\n",
      "Epoch number 35\n",
      " Current loss 0.9149132966995239\n",
      "\n",
      "Epoch number 35\n",
      " Current loss 1.4451391696929932\n",
      "\n",
      "Epoch number 36\n",
      " Current loss 0.6727102994918823\n",
      "\n",
      "Epoch number 36\n",
      " Current loss 0.8991498947143555\n",
      "\n",
      "Epoch number 36\n",
      " Current loss 0.9037059545516968\n",
      "\n",
      "Epoch number 37\n",
      " Current loss 0.6994925141334534\n",
      "\n",
      "Epoch number 37\n",
      " Current loss 0.9768115282058716\n",
      "\n",
      "Epoch number 37\n",
      " Current loss 0.6515593528747559\n",
      "\n",
      "Epoch number 38\n",
      " Current loss 1.0654844045639038\n",
      "\n",
      "Epoch number 38\n",
      " Current loss 1.410474181175232\n",
      "\n",
      "Epoch number 38\n",
      " Current loss 0.8825151920318604\n",
      "\n",
      "Epoch number 39\n",
      " Current loss 0.9895228147506714\n",
      "\n",
      "Epoch number 39\n",
      " Current loss 0.9195060133934021\n",
      "\n",
      "Epoch number 39\n",
      " Current loss 1.4876556396484375\n",
      "\n",
      "Epoch number 40\n",
      " Current loss 1.1376965045928955\n",
      "\n",
      "Epoch number 40\n",
      " Current loss 1.1699614524841309\n",
      "\n",
      "Epoch number 40\n",
      " Current loss 0.7165900468826294\n",
      "\n",
      "Epoch number 41\n",
      " Current loss 1.1458184719085693\n",
      "\n",
      "Epoch number 41\n",
      " Current loss 0.7691662311553955\n",
      "\n",
      "Epoch number 41\n",
      " Current loss 0.960975170135498\n",
      "\n",
      "Epoch number 42\n",
      " Current loss 0.8857502937316895\n",
      "\n",
      "Epoch number 42\n",
      " Current loss 0.9020246267318726\n",
      "\n",
      "Epoch number 42\n",
      " Current loss 1.1035716533660889\n",
      "\n",
      "Epoch number 43\n",
      " Current loss 1.1649463176727295\n",
      "\n",
      "Epoch number 43\n",
      " Current loss 0.7571936845779419\n",
      "\n",
      "Epoch number 43\n",
      " Current loss 1.2504665851593018\n",
      "\n",
      "Epoch number 44\n",
      " Current loss 1.7705258131027222\n",
      "\n",
      "Epoch number 44\n",
      " Current loss 0.8641999959945679\n",
      "\n",
      "Epoch number 44\n",
      " Current loss 1.0434660911560059\n",
      "\n",
      "Epoch number 45\n",
      " Current loss 0.8468179106712341\n",
      "\n",
      "Epoch number 45\n",
      " Current loss 0.5578659772872925\n",
      "\n",
      "Epoch number 45\n",
      " Current loss 1.3881428241729736\n",
      "\n",
      "Epoch number 46\n",
      " Current loss 0.9085778594017029\n",
      "\n",
      "Epoch number 46\n",
      " Current loss 1.1548526287078857\n",
      "\n",
      "Epoch number 46\n",
      " Current loss 1.1417882442474365\n",
      "\n",
      "Epoch number 47\n",
      " Current loss 1.008310079574585\n",
      "\n",
      "Epoch number 47\n",
      " Current loss 0.9838432669639587\n",
      "\n",
      "Epoch number 47\n",
      " Current loss 1.0902801752090454\n",
      "\n",
      "Epoch number 48\n",
      " Current loss 1.0450547933578491\n",
      "\n",
      "Epoch number 48\n",
      " Current loss 0.9859424829483032\n",
      "\n",
      "Epoch number 48\n",
      " Current loss 0.9757614731788635\n",
      "\n",
      "Epoch number 49\n",
      " Current loss 0.9730592966079712\n",
      "\n",
      "Epoch number 49\n",
      " Current loss 1.1761401891708374\n",
      "\n",
      "Epoch number 49\n",
      " Current loss 0.9130613207817078\n",
      "\n",
      "Epoch number 50\n",
      " Current loss 1.0164870023727417\n",
      "\n",
      "Epoch number 50\n",
      " Current loss 1.03151273727417\n",
      "\n",
      "Epoch number 50\n",
      " Current loss 1.1943273544311523\n",
      "\n",
      "Epoch number 51\n",
      " Current loss 0.8394421935081482\n",
      "\n",
      "Epoch number 51\n",
      " Current loss 1.4601916074752808\n",
      "\n",
      "Epoch number 51\n",
      " Current loss 1.1310391426086426\n",
      "\n",
      "Epoch number 52\n",
      " Current loss 0.9153709411621094\n",
      "\n",
      "Epoch number 52\n",
      " Current loss 0.9874646067619324\n",
      "\n",
      "Epoch number 52\n",
      " Current loss 0.5127392411231995\n",
      "\n",
      "Epoch number 53\n",
      " Current loss 1.2573902606964111\n",
      "\n",
      "Epoch number 53\n",
      " Current loss 0.7124701142311096\n",
      "\n",
      "Epoch number 53\n",
      " Current loss 1.0611039400100708\n",
      "\n",
      "Epoch number 54\n",
      " Current loss 0.9989537000656128\n",
      "\n",
      "Epoch number 54\n",
      " Current loss 1.2413594722747803\n",
      "\n",
      "Epoch number 54\n",
      " Current loss 0.9151065945625305\n",
      "\n",
      "Epoch number 55\n",
      " Current loss 0.6239752769470215\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 55\n",
      " Current loss 1.3621082305908203\n",
      "\n",
      "Epoch number 55\n",
      " Current loss 1.79432213306427\n",
      "\n",
      "Epoch number 56\n",
      " Current loss 1.0551176071166992\n",
      "\n",
      "Epoch number 56\n",
      " Current loss 1.4169350862503052\n",
      "\n",
      "Epoch number 56\n",
      " Current loss 1.3094457387924194\n",
      "\n",
      "Epoch number 57\n",
      " Current loss 1.180309534072876\n",
      "\n",
      "Epoch number 57\n",
      " Current loss 0.7546892166137695\n",
      "\n",
      "Epoch number 57\n",
      " Current loss 1.2692431211471558\n",
      "\n",
      "Epoch number 58\n",
      " Current loss 0.9461232423782349\n",
      "\n",
      "Epoch number 58\n",
      " Current loss 0.9769003987312317\n",
      "\n",
      "Epoch number 58\n",
      " Current loss 1.3975038528442383\n",
      "\n",
      "Epoch number 59\n",
      " Current loss 1.070468544960022\n",
      "\n",
      "Epoch number 59\n",
      " Current loss 0.6246205568313599\n",
      "\n",
      "Epoch number 59\n",
      " Current loss 1.176556944847107\n",
      "\n",
      "Epoch number 60\n",
      " Current loss 1.233526349067688\n",
      "\n",
      "Epoch number 60\n",
      " Current loss 0.9441483616828918\n",
      "\n",
      "Epoch number 60\n",
      " Current loss 0.825726330280304\n",
      "\n",
      "Epoch number 61\n",
      " Current loss 1.247135877609253\n",
      "\n",
      "Epoch number 61\n",
      " Current loss 0.9928355813026428\n",
      "\n",
      "Epoch number 61\n",
      " Current loss 1.3753416538238525\n",
      "\n",
      "Epoch number 62\n",
      " Current loss 1.2031874656677246\n",
      "\n",
      "Epoch number 62\n",
      " Current loss 1.0401581525802612\n",
      "\n",
      "Epoch number 62\n",
      " Current loss 1.0451571941375732\n",
      "\n",
      "Epoch number 63\n",
      " Current loss 1.156790852546692\n",
      "\n",
      "Epoch number 63\n",
      " Current loss 0.6801651120185852\n",
      "\n",
      "Epoch number 63\n",
      " Current loss 0.6998047232627869\n",
      "\n",
      "Epoch number 64\n",
      " Current loss 1.2115144729614258\n",
      "\n",
      "Epoch number 64\n",
      " Current loss 1.2291662693023682\n",
      "\n",
      "Epoch number 64\n",
      " Current loss 0.9986470937728882\n",
      "\n",
      "Epoch number 65\n",
      " Current loss 1.0226730108261108\n",
      "\n",
      "Epoch number 65\n",
      " Current loss 0.6318367123603821\n",
      "\n",
      "Epoch number 65\n",
      " Current loss 0.9142120480537415\n",
      "\n",
      "Epoch number 66\n",
      " Current loss 1.2027944326400757\n",
      "\n",
      "Epoch number 66\n",
      " Current loss 1.041940450668335\n",
      "\n",
      "Epoch number 66\n",
      " Current loss 1.496348261833191\n",
      "\n",
      "Epoch number 67\n",
      " Current loss 1.2252922058105469\n",
      "\n",
      "Epoch number 67\n",
      " Current loss 1.2436671257019043\n",
      "\n",
      "Epoch number 67\n",
      " Current loss 0.7079610228538513\n",
      "\n",
      "Epoch number 68\n",
      " Current loss 1.1281929016113281\n",
      "\n",
      "Epoch number 68\n",
      " Current loss 1.0740857124328613\n",
      "\n",
      "Epoch number 68\n",
      " Current loss 1.2286412715911865\n",
      "\n",
      "Epoch number 69\n",
      " Current loss 1.2395243644714355\n",
      "\n",
      "Epoch number 69\n",
      " Current loss 1.1053454875946045\n",
      "\n",
      "Epoch number 69\n",
      " Current loss 0.9022534489631653\n",
      "\n",
      "Epoch number 70\n",
      " Current loss 1.0549888610839844\n",
      "\n",
      "Epoch number 70\n",
      " Current loss 1.12870192527771\n",
      "\n",
      "Epoch number 70\n",
      " Current loss 0.9093497395515442\n",
      "\n",
      "Epoch number 71\n",
      " Current loss 0.8703641295433044\n",
      "\n",
      "Epoch number 71\n",
      " Current loss 0.9606389999389648\n",
      "\n",
      "Epoch number 71\n",
      " Current loss 1.1410094499588013\n",
      "\n",
      "Epoch number 72\n",
      " Current loss 1.25760018825531\n",
      "\n",
      "Epoch number 72\n",
      " Current loss 0.8137664794921875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,Config.train_number_epochs):\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        img0, img1, label = data\n",
    "        img0, img1, label = Variable(img0).cuda(), Variable(img1).cuda() , Variable(label).cuda()\n",
    "        output1, output2 = net(img0,img1)\n",
    "        net.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        loss_contrastive = criterion(output1,output2,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        if i %10 == 0 :\n",
    "            print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.data[0]))\n",
    "            iteration_number +=10\n",
    "            counter.append(iteration_number)\n",
    "            loss_history.append(loss_contrastive.data[0])\n",
    "show_plot(counter,loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple testing\n",
    "The last 3 subjects were held out from the training, and will be used to test. The Distance between each image pair denotes the degree of similarity the model found between the two images. Less means it found more similar, while higher values indicate it found them to be dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n",
    "                                        transform=transforms.Compose([transforms.Scale((100,100)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)\n",
    "\n",
    "test_dataloader = DataLoader(siamese_dataset,num_workers=6,batch_size=1,shuffle=True)\n",
    "\n",
    "for data in test_dataloader:\n",
    "    x0, x1, label2, x0label0, x1label1 = data\n",
    "    print(label2)\n",
    "    concatenated = torch.cat((x0,x1),0)\n",
    "    \n",
    "    output1,output2 = net(Variable(x0).cuda(),Variable(x1).cuda())\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2) \n",
    "    imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(euclidean_distance.cpu().data.numpy()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
