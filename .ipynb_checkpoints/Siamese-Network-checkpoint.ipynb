{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Shot Learning with Siamese Networks\n",
    "\n",
    "This is the jupyter notebook that accompanies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "All the imports are defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Set of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(img,text,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Class\n",
    "A simple class to manage configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    training_dir = \"/home/sangram/Downloads/Photos/training\"\n",
    "    testing_dir = \"/home/sangram/Downloads/Photos/testing\"\n",
    "    train_batch_size = 16\n",
    "    train_number_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "This dataset generates a pair of images. 0 for geniune pair and 1 for imposter pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training DataSet Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        #img0 = img0.convert(\"L\")\n",
    "        #img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Siamese Bagging DataSet Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaggingCoupledSiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #img0_worand = self.imageFolderDataset.imgs\n",
    "        #print img0_tuple\n",
    "        #print img0_worand\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32)), int(img0_tuple[1]), int(img1_tuple[1]) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Image Folder Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_dataset = dset.ImageFolder(root=Config.training_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "siamese_dataset = BaggingCoupledSiameseNetworkDataset(imageFolderDataset=folder_dataset,\n",
    "                                        transform=transforms.Compose([transforms.Scale((100,100)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Net Definition\n",
    "We will use a standard convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(3, 4, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Dropout2d(p=.2),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(4, 8, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout2d(p=.2),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(8, 8, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout2d(p=.2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(8*100*100, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 20))\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=False,\n",
    "                        num_workers=8,\n",
    "                        batch_size=Config.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = SiameseNetwork().cuda()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.0005 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      " Current loss 0.64560765028\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 3.79493761063\n",
      "\n",
      "Epoch number 0\n",
      " Current loss 3.47908258438\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 4.55263996124\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 2.19575929642\n",
      "\n",
      "Epoch number 1\n",
      " Current loss 1.75908172131\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.31400609016\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 0.913601279259\n",
      "\n",
      "Epoch number 2\n",
      " Current loss 1.18664169312\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 2.08658957481\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 0.77032327652\n",
      "\n",
      "Epoch number 3\n",
      " Current loss 0.873049557209\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.27423381805\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.16461038589\n",
      "\n",
      "Epoch number 4\n",
      " Current loss 1.42944848537\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.27505421638\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 0.865567028522\n",
      "\n",
      "Epoch number 5\n",
      " Current loss 1.3148881197\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 0.958090722561\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.0565983057\n",
      "\n",
      "Epoch number 6\n",
      " Current loss 1.10817301273\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.55689668655\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.14655435085\n",
      "\n",
      "Epoch number 7\n",
      " Current loss 1.35888886452\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.03975224495\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 0.823166668415\n",
      "\n",
      "Epoch number 8\n",
      " Current loss 1.50325739384\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.0774140358\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 1.29790842533\n",
      "\n",
      "Epoch number 9\n",
      " Current loss 0.716360449791\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 1.1987811327\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 0.928607642651\n",
      "\n",
      "Epoch number 10\n",
      " Current loss 0.955154776573\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 2.32609438896\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 0.907661378384\n",
      "\n",
      "Epoch number 11\n",
      " Current loss 1.36626076698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,Config.train_number_epochs):\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        img0, img1, label,_,_= data\n",
    "#        print _1,'--',_2\n",
    "        img0, img1, label = Variable(img0).cuda(), Variable(img1).cuda() , Variable(label).cuda()\n",
    "        output1, output2 = net(img0,img1)\n",
    "        net.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        loss_contrastive = criterion(output1,output2,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        if i %10 == 0 :\n",
    "            print \"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.data[0])\n",
    "            iteration_number +=10\n",
    "            counter.append(iteration_number)\n",
    "            loss_history.append(loss_contrastive.data[0])\n",
    "show_plot(counter,loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple testing\n",
    "The last 3 subjects were held out from the training, and will be used to test. The Distance between each image pair denotes the degree of similarity the model found between the two images. Less means it found more similar, while higher values indicate it found them to be dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\n",
    "siamese_dataset_bagging_test = BaggingCoupledSiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n",
    "                                        transform=transforms.Compose([transforms.Scale((100,100)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)\n",
    "\n",
    "test_dataloader = DataLoader(siamese_dataset_bagging_test,num_workers=6,batch_size=1,shuffle=True)\n",
    "\n",
    "dictionary_head = {0:'Benign', 1:'InSitu', 2:'Invasive',3:'Normal'}\n",
    "\n",
    "for index_head,data in enumerate(test_dataloader):\n",
    "    x0_head, x1_head, _, x0label0_head, x1label1_head = data\n",
    "    true_value = dictionary_head[x0label0_head]\n",
    "    \n",
    "    class_votes_0 = []\n",
    "    class_votes_1 = []\n",
    "    class_votes_2 = []\n",
    "    class_votes_3 = []\n",
    "    if(index_head==1):\n",
    "        break\n",
    "    for index_sample,data_sample in enumerate(train_dataloader):\n",
    "        x0_sample, x1_sample, _, x0label_sample, x1label_sample = data_sample        \n",
    "        output1,output2 = net(Variable(x0_head).cuda(),Variable(x0_sample).cuda())\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        if x1label_sample == 0:\n",
    "            class_votes_0.append(euclidean_distance.cpu().data.numpy()[0][0])\n",
    "        elif x1label_sample == 1:\n",
    "            class_votes_1.append(euclidean_distance.cpu().data.numpy()[0][0])\n",
    "        elif x1label_sample == 2:\n",
    "            class_votes_2.append(euclidean_distance.cpu().data.numpy()[0][0])\n",
    "        elif x1label_sample == 3:\n",
    "            class_votes_3.append(euclidean_distance.cpu().data.numpy()[0][0])\n",
    "        \n",
    "    prediction_0 = sum(class_votes_0)/len(class_votes_0)\n",
    "    prediction_1 = sum(class_votes_1)/len(class_votes_1)\n",
    "    prediction_2 = sum(class_votes_2)/len(class_votes_2)\n",
    "    prediction_3 = sum(class_votes_3)/len(class_votes_3)\n",
    "    \n",
    "    dictionary = {'Benign': prediction_0, 'InSitu': prediction_1, 'Invasive': prediction_2,  'Normal': prediction_3}\n",
    "    final_prediction = max(dictionary, key=d.get)\n",
    "    print(true_value,final_prediction)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
